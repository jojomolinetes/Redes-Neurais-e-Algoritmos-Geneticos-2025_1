{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fb2702",
   "metadata": {},
   "source": [
    "# Um momento, por favor!\n",
    "\n",
    "**Nome: Joana de Medeiros Oliveira Hulse Molinete**\n",
    "\n",
    "### Introdução:\n",
    "No presente notebook, busco implementar, treinar e testar o otimizador de Descida do Gradiente com Momento em uma rede neural, utilizando como base o notebook com uma rede neural feita em Python puro disponibilizado pelo professor discente Daniel Cassar. Pretendo, além de aplicar os conhecimentos adquiridos sobre otimização de redes neurais, elucidar o método de funcionamento do Gradient Descent with Momentum e o motivo por trás de sua eficácia na otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2189ba6",
   "metadata": {},
   "source": [
    "### *Gradient Descent* e *Gradient Descent with Momentum*\n",
    "Na descida de gradiente normal, o gradiente da função de perda é calculado em relação aos parâmetros, e atualiza os parâmetros na direção contrária do gradiente, visando diminuir o erro do modelo. O funcionamento da descida de gradiente segue a seguinte fórmula:\n",
    "\n",
    "$$\n",
    "p_{t+1} = p_{t} - \\alpha \\cdot \\nabla L(p_{t})\n",
    "$$\n",
    "\n",
    "Onde $p_{t}$ são os parâmetros do modelo, $\\alpha$ é a taxa de aprendizado e $\\nabla L(p_{t})$ é o gradiente da função de perda.\n",
    "A descida de gradiente é aplicada no treinamento original do notebook didático utilizado como base do presente notebook.\n",
    "\n",
    "Já a descida de gradiente com momentum faz o mesmo processo, mas adiciona o fator momentum da física, que basicamente \"acumula\" uma velocidade a cada nova iteração da rede, fazendo com que o gradiente converja mais rapidamente para seu mínimo, já que tem influência dos gradientes passados a cada nova atualização de parâmetros, e ainda evita falsos mínimos (mínimos locais). A fórmula difere um pouco da tradicional pois adicionamos o fator momento:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} - \\alpha \\cdot \\nabla J(\\theta_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} + v_t\n",
    "$$\n",
    "\n",
    "\n",
    "Onde $\\beta$ é o fator *momentum*, $\\alpha$ é a taxa de aprendizado, v_t é a velocidade acumulada no tempo _t_, $\\nabla J(\\theta_t)$ é o gradiente da função de perda e $\\theta$ são os parâmetros atualizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e2945",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60432d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana24003\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "semente_aleatoria = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e442d8",
   "metadata": {},
   "source": [
    "### Rede neural:\n",
    "\n",
    "Como dito anteriormente, a rede neural original foi disponibilizada pelo professor Daniel Cassar [1] e otimizada utilizando o Gradient Descent with Momentum. \n",
    "\n",
    "Foi necessário a adição de um novo atributo que armazenasse o histórico de atualizações (\"lembrasse o passado\") de cada parâmetro conforme a rede neural iterasse durante o treinamento. O novo atributo, chamado de velocidades, está na classe MLP, e inicialmente cria uma lista preenchida com zeros, de acordo com o número de parâmetros da rede neural. Conforme a rede neural trabalha, atribuindo os pesos e viéses em cada neurônio, a lista de parâmetros que inicialmente tinha apenas zeros vai sendo preenchida pelos vetores de velocidade de cada parâmetro a cada atualização.\n",
    "\n",
    "O conceito de momentum foi implementado no treinamento da rede, na parte de atualização de parâmetros, utilizando a seguinte fórmula:\n",
    "\n",
    "$$v_t = \\beta v_{t-1} - \\alpha \\nabla J(\\theta_t)$$\n",
    "$$\\theta_t = \\theta_{t-1} + v_t$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a6f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_rmse(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    soma_erro = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        erro = y_pred[i] - y_true[i]\n",
    "        erro_quadrado = erro ** 2\n",
    "        soma_erro += erro_quadrado\n",
    "\n",
    "    mse = soma_erro / n\n",
    "    rmse = mse ** (1/2)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "class Valor:\n",
    "    def __init__(self, data, progenitor=(), operador_mae=\"\", rotulo=\"\"):\n",
    "        self.data = data\n",
    "        self.progenitor = progenitor\n",
    "        self.operador_mae = operador_mae\n",
    "        self.rotulo = rotulo\n",
    "        self.grad = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Valor(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self + outro_valor.\"\"\"\n",
    "        \n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data + outro_valor.data\n",
    "        operador_mae = \"+\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_adicao():\n",
    "            self.grad += resultado.grad\n",
    "            outro_valor.grad += resultado.grad\n",
    "            \n",
    "        resultado.propagar = propagar_adicao\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def __mul__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self * outro_valor.\"\"\"\n",
    "        \n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data * outro_valor.data\n",
    "        operador_mae = \"*\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_multiplicacao():\n",
    "            self.grad += resultado.grad * outro_valor.data # grad_filho * derivada filho em relação a mãe\n",
    "            outro_valor.grad += resultado.grad * self.data\n",
    "            \n",
    "        resultado.propagar = propagar_multiplicacao\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"Realiza a operação: exp(self)\"\"\"\n",
    "        progenitor = (self, )\n",
    "        data = math.exp(self.data)\n",
    "        operador_mae = \"exp\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_exp():\n",
    "            self.grad += resultado.grad * data \n",
    "        \n",
    "        resultado.propagar = propagar_exp\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def __pow__(self, expoente):\n",
    "        \"\"\"Realiza a operação: self ** expoente\"\"\"\n",
    "        assert isinstance(expoente, (int, float))\n",
    "        progenitor = (self, )\n",
    "        data = self.data ** expoente\n",
    "        operador_mae = f\"**{expoente}\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_pow():\n",
    "            self.grad += resultado.grad * (expoente * self.data ** (expoente - 1))\n",
    "        \n",
    "        resultado.propagar = propagar_pow\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def __truediv__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self / outro_valor\"\"\"\n",
    "        return self * outro_valor ** (-1)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"Realiza a operação: -self\"\"\"\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self - outro_valor\"\"\"\n",
    "        return self + (-outro_valor)\n",
    "    \n",
    "    def __radd__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: outro_valor + self\"\"\"\n",
    "        return self + outro_valor\n",
    "    \n",
    "    def __rmul__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: outro_valor * self\"\"\"\n",
    "        return self * outro_valor\n",
    "    \n",
    "    def sig(self):\n",
    "        \"\"\"Realiza a operação: exp(self) / (exp(self) + 1)\"\"\"\n",
    "        return self.exp() / (self.exp() + 1)\n",
    "    \n",
    "    def propagar(self):\n",
    "        pass\n",
    "    \n",
    "    def propagar_tudo(self):\n",
    "        \n",
    "        self.grad = 1\n",
    "        \n",
    "        ordem_topologica = []\n",
    "        \n",
    "        visitados = set()\n",
    "\n",
    "        def constroi_ordem_topologica(v):\n",
    "            if v not in visitados:\n",
    "                visitados.add(v)\n",
    "                for progenitor in v.progenitor:\n",
    "                    constroi_ordem_topologica(progenitor)\n",
    "                ordem_topologica.append(v)\n",
    "\n",
    "        constroi_ordem_topologica(self)\n",
    "        \n",
    "        for vertice in reversed(ordem_topologica):\n",
    "            vertice.propagar()\n",
    "\n",
    "            \n",
    "class Neuronio:\n",
    "    def __init__(self, num_dados_entrada):\n",
    "        self.vies = Valor(random.uniform(-1, 1))\n",
    "        \n",
    "        self.pesos = []\n",
    "        for i in range(num_dados_entrada):\n",
    "            self.pesos.append(Valor(random.uniform(-1, 1)))\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        assert len(x) == len(self.pesos)\n",
    "        \n",
    "        soma = 0\n",
    "        for info_entrada, peso_interno in zip(x, self.pesos):\n",
    "            soma += info_entrada * peso_interno\n",
    "            \n",
    "        soma += self.vies  \n",
    "        dado_de_saida = soma.sig()\n",
    "        \n",
    "        return dado_de_saida       \n",
    "    \n",
    "    def parametros(self):\n",
    "        return self.pesos + [self.vies]\n",
    "    \n",
    "    \n",
    "class Camada:\n",
    "    def __init__(self, num_neuronios, num_dados_entrada):\n",
    "        neuronios = []\n",
    "        \n",
    "        for _ in range(num_neuronios):\n",
    "            neuronio = Neuronio(num_dados_entrada)\n",
    "            neuronios.append(neuronio)\n",
    "            \n",
    "        self.neuronios = neuronios     \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        dados_de_saida = []\n",
    "        \n",
    "        for neuronio in self.neuronios:\n",
    "            informacao = neuronio(x)\n",
    "            dados_de_saida.append(informacao)\n",
    "            \n",
    "        if len(dados_de_saida) == 1:\n",
    "            return dados_de_saida[0]\n",
    "        else:        \n",
    "            return dados_de_saida  \n",
    "    \n",
    "    def parametros(self):\n",
    "        params = []\n",
    "        \n",
    "        for neuronio in self.neuronios:\n",
    "            params_neuronio = neuronio.parametros()\n",
    "            params.extend(params_neuronio)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, num_dados_entrada, num_neuronios_por_camada):\n",
    "        \n",
    "        percurso = [num_dados_entrada] + num_neuronios_por_camada\n",
    "        \n",
    "        camadas = []\n",
    "        \n",
    "        for i in range(len(num_neuronios_por_camada)):\n",
    "            camada = Camada(num_neuronios_por_camada[i], percurso[i])\n",
    "            camadas.append(camada)\n",
    "            \n",
    "        self.camadas = camadas\n",
    "        \n",
    "        # novo atributo!\n",
    "        self.velocidades = [0 for _ in self.parametros()] # precisamos que o momentum seja uma lista de zeros do mesmo tamanho que os parametros\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for camada in self.camadas:\n",
    "            x = camada(x)\n",
    "        return x\n",
    "    \n",
    "    def parametros(self):\n",
    "        params = []\n",
    "        \n",
    "        for camada in self.camadas:\n",
    "            parametros_camada = camada.parametros()\n",
    "            params.extend(parametros_camada)\n",
    "            \n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee2d4e",
   "metadata": {},
   "source": [
    "## Testando a rede neural com o dataset Boston Housing:\n",
    "\n",
    "Para que seja possível observar se a implementação do otimizador Gradient Descent with Momentum foi efetiva, vamos testar a rede neural em um dataset. Aqui, o objetivo da nossa rede neural é prever o preço das casas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea7e5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
       "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
       "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
       "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
       "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
       "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
       "\n",
       "     ptratio       b  lstat  medv  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/selva86/datasets/refs/heads/master/BostonHousing.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04072f",
   "metadata": {},
   "source": [
    "### Separando os dados:\n",
    "\n",
    "Vamos separar nosso dataset entre os dados de treino e de teste. Aqui, separamos 80% dos dados para treino, e 20% dos dados para teste. Além disso, as features utilizadas foram \"indus\" (proporção de acres destinados a negócios não comerciais), \"rm\" (número médio de cômodos por residência), \"crim\" (taxa de criminalidade per capita por cidade) e \"lstat\" (percentual da população de nível socioeconômico baixo), enquanto nosso target foi \"medv\", que é o valor médio do imóvel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb05395",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_teste = 0.2\n",
    "\n",
    "FEATURES = ['indus', 'rm', 'crim', 'lstat']\n",
    "TARGET = ['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed60dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df.index\n",
    "indices_treino, indices_teste = train_test_split(\n",
    "    indices, test_size = tamanho_teste, random_state = semente_aleatoria\n",
    ")\n",
    "\n",
    "df_treino = df.loc[indices_treino]\n",
    "df_teste = df.loc[indices_teste]\n",
    "\n",
    "X_treino = df_treino.reindex(FEATURES, axis=1).values\n",
    "y_treino = df_treino.reindex(TARGET, axis=1).values.ravel()\n",
    "\n",
    "X_teste = df_teste.reindex(FEATURES, axis=1).values\n",
    "y_teste = df_teste.reindex(TARGET, axis=1).values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4688be5",
   "metadata": {},
   "source": [
    "Normalizando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5bba748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.05     6.416    0.09178  9.04   ]\n",
      " [ 3.41     6.417    0.04684  8.81   ]\n",
      " [ 3.97     6.842    0.65665  6.9    ]\n",
      " ...\n",
      " [18.1      5.713    6.96215 17.11   ]\n",
      " [10.81     6.065    0.09164  5.52   ]\n",
      " [18.1      6.436    5.58107 16.22   ]]\n",
      "\n",
      "[[-1.04197113  0.1916685  -0.4466585  -0.50585907]\n",
      " [-1.13864686  0.19307859 -0.45277542 -0.53807131]\n",
      " [-1.05405559  0.79236987 -0.36977232 -0.80557297]\n",
      " ...\n",
      " [ 1.08036324 -0.79962978  0.48848856  0.62437044]\n",
      " [-0.02083374 -0.30327559 -0.44667755 -0.99884642]\n",
      " [ 1.08036324  0.21987044  0.30050554  0.49972307]]\n"
     ]
    }
   ],
   "source": [
    "normalizador = StandardScaler()\n",
    "\n",
    "X_treino_norm = normalizador.fit_transform(X_treino)\n",
    "X_teste_norm = normalizador.fit_transform(X_teste)\n",
    "\n",
    "y_treino_norm = normalizador.fit_transform(y_treino.reshape(-1,1))\n",
    "y_teste_norm = normalizador.fit_transform(y_teste.reshape(-1,1))\n",
    "\n",
    "print(X_treino)\n",
    "print()\n",
    "print(X_treino_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df36ceb",
   "metadata": {},
   "source": [
    "Definimos a arquitetura da rede neural tendo como entrada 4 valores (\"INDUS\", \"RM\", \"CRIM\", \"LSTAT\") e contendo 2 camadas ocultas, a primeira com 3 neurônios e a última com 2 neurônios. A saída é um único valor, porque é uma rede regressora que busca estimar um valor depois do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d04107",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 4\n",
    "NUM_DADOS_DE_SAIDA = 1    \n",
    "CAMADAS_OCULTAS = [3, 2]  \n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]\n",
    "\n",
    "minha_mlp_GDM = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646532a0",
   "metadata": {},
   "source": [
    "### Treinando a rede neural:\n",
    "\n",
    "Aqui, introduzimos efetivamente a fórmula do Gradient Descent with Momentum, na etapa de atualização de parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed23d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 0, Loss: [1.42153507]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana24003\\AppData\\Local\\Temp\\ipykernel_30264\\673356561.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  data = math.exp(self.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 10, Loss: [1.34772348]\n",
      "Época: 20, Loss: [1.2258148]\n",
      "Época: 30, Loss: [1.13138805]\n",
      "Época: 40, Loss: [1.07825621]\n",
      "Época: 50, Loss: [1.05104413]\n",
      "Época: 60, Loss: [1.03653035]\n",
      "Época: 70, Loss: [1.02806284]\n",
      "Época: 80, Loss: [1.02264163]\n",
      "Época: 90, Loss: [1.01888691]\n",
      "Época: 100, Loss: [1.01612346]\n",
      "Época: 110, Loss: [1.01399525]\n",
      "Época: 120, Loss: [1.01229995]\n",
      "Época: 130, Loss: [1.01091438]\n",
      "Época: 140, Loss: [1.00975895]\n",
      "Época: 150, Loss: [1.00877964]\n",
      "Época: 160, Loss: [1.00793835]\n",
      "Época: 170, Loss: [1.00720733]\n",
      "Época: 180, Loss: [1.00656582]\n",
      "Época: 190, Loss: [1.00599798]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 200\n",
    "TAXA_DE_APRENDIZADO = 0.01\n",
    "fator_momentum = 0.9\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for exemplo in X_treino_norm:\n",
    "        previsao = minha_mlp_GDM(exemplo)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # loss\n",
    "    erros = []\n",
    "    for yt, yp in zip(y_treino_norm, y_pred):\n",
    "        residuo = yp - yt\n",
    "        erro_quadratico = residuo ** 2\n",
    "        erros.append(erro_quadratico)        \n",
    "    loss = sum(erros)/len(erros)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp_GDM.parametros():\n",
    "        p.grad = 0\n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza parâmetros com momentum\n",
    "    for indice, parametro in enumerate(minha_mlp_GDM.parametros()):\n",
    "        v_antiga = minha_mlp_GDM.velocidades[indice]\n",
    "        grad = parametro.grad\n",
    "        nova_v = fator_momentum * v_antiga - TAXA_DE_APRENDIZADO * grad\n",
    "        minha_mlp_GDM.velocidades[indice] = nova_v \n",
    "        parametro.data += nova_v\n",
    "\n",
    "    # mostra resultado (opcional)\n",
    "    if epoca % 10 == 0:\n",
    "        print(f'Época: {epoca}, Loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c926db",
   "metadata": {},
   "source": [
    "### Testando a previsão da rede:\n",
    "\n",
    "Vejamos então como nossa rede neural se sai ao tentar prever valores reais do nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecafd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana24003\\AppData\\Local\\Temp\\ipykernel_30264\\673356561.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  data = math.exp(self.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valor(data=0.08963313241634757), Valor(data=0.09619815892240789), Valor(data=0.09784902209493065), Valor(data=0.0914494959242233), Valor(data=0.08998535919608203), Valor(data=0.0902168147522078), Valor(data=0.0911429633959693), Valor(data=0.09151739821002777), Valor(data=0.0917158539540397), Valor(data=0.09044409211611965), Valor(data=0.09237911958135923), Valor(data=0.09750681112023592), Valor(data=0.09422857387587706), Valor(data=0.09754678044073369), Valor(data=0.09164647842053406), Valor(data=0.09087862748062209), Valor(data=0.09036678325520028), Valor(data=0.09761214288812993), Valor(data=0.09748310252988097), Valor(data=0.09386267794513442), Valor(data=0.09223615827951856), Valor(data=0.09128825247520884), Valor(data=0.0924752201667965), Valor(data=0.09166287726979168), Valor(data=0.09208991806346445), Valor(data=0.09062896545659287), Valor(data=0.0919399176778554), Valor(data=0.09156226986678297), Valor(data=0.09689854538026685), Valor(data=0.09148437994055285), Valor(data=0.09766500069891239), Valor(data=0.09550997205206173), Valor(data=0.0948306431082174), Valor(data=0.09525819885042379), Valor(data=0.09673022191290688), Valor(data=0.09536384589014323), Valor(data=0.09071254587746685), Valor(data=0.09541810248124716), Valor(data=0.09186912437216593), Valor(data=0.09762662229638039), Valor(data=0.0942527760495296), Valor(data=0.0972322937927594), Valor(data=0.09299685561940596), Valor(data=0.09584669956418147), Valor(data=0.09058458730332351), Valor(data=0.0975287192006312), Valor(data=0.09785763816545207), Valor(data=0.09107097827565772), Valor(data=0.09065564714882281), Valor(data=0.0973053591126566), Valor(data=0.0977029232986299), Valor(data=0.09626318073314248), Valor(data=0.09554732384414234), Valor(data=0.0934550553581186), Valor(data=0.09662657423652508), Valor(data=0.09691811544015665), Valor(data=0.09476046603594776), Valor(data=0.09225880095190454), Valor(data=0.08963601421389973), Valor(data=0.09120315139588496), Valor(data=0.09159944851642862), Valor(data=0.09723976338772082), Valor(data=0.09652072465312142), Valor(data=0.09766978737729431), Valor(data=0.09758497097859412), Valor(data=0.09210752661504795), Valor(data=0.096296409485033), Valor(data=0.09119625318798498), Valor(data=0.09484830173780152), Valor(data=0.09141517139802804), Valor(data=0.09666637890076916), Valor(data=0.0976230250381863), Valor(data=0.09760112338826472), Valor(data=0.0963121648978943), Valor(data=0.09444816979238603), Valor(data=0.09677467795820828), Valor(data=0.09692064356975029), Valor(data=0.09653607505143948), Valor(data=0.09660957619281263), Valor(data=0.0951839199110686), Valor(data=0.09501819793241031), Valor(data=0.09763437661036631), Valor(data=0.09666020428471818), Valor(data=0.09007659543461947), Valor(data=0.09714060927684921), Valor(data=0.09517004005695914), Valor(data=0.09090156815612206), Valor(data=0.09580075724969545), Valor(data=0.09019919472490728), Valor(data=0.08916296630899531), Valor(data=0.09388135177454313), Valor(data=0.09150582909339312), Valor(data=0.09775352786946676), Valor(data=0.09691640187545086), Valor(data=0.09791017325214603), Valor(data=0.09135914943555727), Valor(data=0.08955961426453064), Valor(data=0.09671450500676718), Valor(data=0.08961103830945506), Valor(data=0.09092152732858295), Valor(data=0.09684330512578335), Valor(data=0.08967769710253189)]\n",
      "\n",
      "RMSE com o otimizador Gradient Descent with Momentum: Valor(data=[1.00253639])\n"
     ]
    }
   ],
   "source": [
    "y_pred_GDM = []\n",
    "\n",
    "for exemplo in X_teste_norm:\n",
    "    previsao = minha_mlp_GDM(exemplo)\n",
    "    y_pred_GDM.append(previsao)\n",
    "\n",
    "print(y_pred_GDM)\n",
    "print()\n",
    "\n",
    "RMSE_GDM = calcular_rmse(y_teste_norm, y_pred_GDM)\n",
    "print(f\"RMSE com o otimizador Gradient Descent with Momentum: {RMSE_GDM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2746f5",
   "metadata": {},
   "source": [
    "### Treinando e testando a rede com Gradient Descent tradicional:\n",
    "\n",
    "Para fins de comparação, vamos treinar e testar uma rede neural otimizada com o Gradient Descent tradicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8452c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 0, Loss: [438.44879373]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana24003\\AppData\\Local\\Temp\\ipykernel_30264\\673356561.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  data = math.exp(self.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 10, Loss: [402.72700865]\n",
      "Época: 20, Loss: [397.67145927]\n",
      "Época: 30, Loss: [380.69426793]\n",
      "Época: 40, Loss: [332.11754703]\n",
      "Época: 50, Loss: [288.73405582]\n",
      "Época: 60, Loss: [270.46231847]\n",
      "Época: 70, Loss: [262.85394315]\n",
      "Época: 80, Loss: [259.0111648]\n",
      "Época: 90, Loss: [256.79006961]\n",
      "Época: 100, Loss: [255.388203]\n",
      "Época: 110, Loss: [254.44299044]\n",
      "Época: 120, Loss: [253.7718194]\n",
      "Época: 130, Loss: [253.27504392]\n",
      "Época: 140, Loss: [252.89465531]\n",
      "Época: 150, Loss: [252.59502431]\n",
      "Época: 160, Loss: [252.35326541]\n",
      "Época: 170, Loss: [252.15411693]\n",
      "Época: 180, Loss: [251.98707504]\n",
      "Época: 190, Loss: [251.84471512]\n"
     ]
    }
   ],
   "source": [
    "minha_mlp_GD = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede)\n",
    "\n",
    "NUM_EPOCAS = 200\n",
    "TAXA_DE_APRENDIZADO = 0.01\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for exemplo in X_treino_norm:\n",
    "        previsao = minha_mlp_GD(exemplo)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # loss\n",
    "    erros = []\n",
    "    for yt, yp in zip(y_treino_norm, y_pred):\n",
    "        residuo = yp - yt\n",
    "        erro_quadratico = residuo ** 2\n",
    "        erros.append(erro_quadratico)        \n",
    "    loss = sum(erros)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp_GD.parametros():\n",
    "        p.grad = 0\n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    for p in minha_mlp_GD.parametros():\n",
    "        p.data = p.data - p.grad * TAXA_DE_APRENDIZADO\n",
    "\n",
    "    # mostra resultado (opcional)\n",
    "    if epoca % 10 == 0:\n",
    "        print(f'Época: {epoca}, Loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175dffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana24003\\AppData\\Local\\Temp\\ipykernel_30264\\673356561.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  data = math.exp(self.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valor(data=0.00194779028910585), Valor(data=0.02383890000942694), Valor(data=0.9468009029251074), Valor(data=0.0022246438574541204), Valor(data=0.0018829305399597836), Valor(data=0.002612977982199203), Valor(data=0.0019733680359636263), Valor(data=0.002672280931297396), Valor(data=0.0029747240831131828), Valor(data=0.0020855339789041197), Valor(data=0.5481035052921844), Valor(data=0.9528967880000554), Valor(data=0.002284699257194837), Valor(data=0.6232170258677272), Valor(data=0.0028249715556526254), Valor(data=0.0019162224985720285), Valor(data=0.001879515303522894), Valor(data=0.08369343061869117), Valor(data=0.010031842677115406), Valor(data=0.015077803941561815), Valor(data=0.008684774520695328), Valor(data=0.002151254832048131), Valor(data=0.46489397120756476), Valor(data=0.002860045425330443), Valor(data=0.0028136073199950123), Valor(data=0.0020346526661393885), Valor(data=0.00618513323849509), Valor(data=0.0022499182247429626), Valor(data=0.03121910830906634), Valor(data=0.0029506083073845713), Valor(data=0.9451262732167461), Valor(data=0.009130577136607655), Valor(data=0.003959567320320759), Valor(data=0.14379404425495929), Valor(data=0.022809514225045912), Valor(data=0.01928061474795717), Valor(data=0.0021294024815579076), Valor(data=0.08709306371375747), Valor(data=0.004741562666529064), Valor(data=0.9748062850149989), Valor(data=0.0020652946118880234), Valor(data=0.003630026761903588), Valor(data=0.975267488671324), Valor(data=0.002466866573016614), Valor(data=0.0018825837687289119), Valor(data=0.9223155955842771), Valor(data=0.98485484247889), Valor(data=0.002577733497171769), Valor(data=0.0022832450671691727), Valor(data=0.9537820705087975), Valor(data=0.39287766952180275), Valor(data=0.0032438549047865963), Valor(data=0.004692989547536503), Valor(data=0.9850901725900183), Valor(data=0.45148526556544866), Valor(data=0.2244344571257348), Valor(data=0.002061825073613126), Valor(data=0.0064721982144406755), Valor(data=0.0019692319221872334), Valor(data=0.008042603817335403), Valor(data=0.0027502182084760975), Valor(data=0.02613424263364547), Valor(data=0.0036446606030230235), Valor(data=0.6536504306047761), Valor(data=0.0701467556244511), Valor(data=0.005516457752118387), Valor(data=0.0063108281889929915), Valor(data=0.0019658074190950143), Valor(data=0.008163227309630837), Valor(data=0.0021645596856248654), Valor(data=0.9014584421504116), Valor(data=0.9827775868968752), Valor(data=0.2960320803858764), Valor(data=0.12944663988127805), Valor(data=0.002478466906298854), Valor(data=0.22517357587911987), Valor(data=0.9855288079351439), Valor(data=0.08053565781525346), Valor(data=0.007094969555254415), Valor(data=0.002755238944137241), Valor(data=0.0025300080466567963), Valor(data=0.985126790970378), Valor(data=0.0038942173887137176), Valor(data=0.0019037429526293649), Valor(data=0.3887720240720478), Valor(data=0.9016430156099892), Valor(data=0.0023502121437293712), Valor(data=0.002612744295884242), Valor(data=0.004047244706874676), Valor(data=0.001886606398087651), Valor(data=0.0022073695787048434), Valor(data=0.002500939780790321), Valor(data=0.9837970699924256), Valor(data=0.9811738520152409), Valor(data=0.06211982318936334), Valor(data=0.0024954913274506177), Valor(data=0.002008346050441413), Valor(data=0.0035214617803689574), Valor(data=0.002346033183620923), Valor(data=0.0019415792244416676), Valor(data=0.9326660842895453), Valor(data=0.0023938751127585)]\n",
      "\n",
      "RMSE com o otimizador Gradient Descent: Valor(data=[0.76836681])\n"
     ]
    }
   ],
   "source": [
    "y_pred_GD = []\n",
    "\n",
    "for exemplo in X_teste_norm:\n",
    "    previsao = minha_mlp_GD(exemplo)\n",
    "    y_pred_GD.append(previsao)\n",
    "\n",
    "print(y_pred_GD)\n",
    "print()\n",
    "\n",
    "RMSE_GD = calcular_rmse(y_teste_norm, y_pred_GD)\n",
    "print(\"RMSE com o otimizador Gradient Descent:\", RMSE_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca70b1",
   "metadata": {},
   "source": [
    "## Conclusões:\n",
    "\n",
    "Após o treinamento das duas redes neurais com o mesmo dataset, e seu teste, podemos notar que o modelo otimizado com o Gradient Descent tradicional obteve um desempenho melhor do que o algoritmo otimizado com o Gradient Descent com o fator Momentum, o que foge um pouco do esperado. \n",
    "\n",
    "No entanto, observamos que durante o treinamento, a rede com GDM apresentou uma loss final ( ~ 1.005) menor em comparação com a GD tradicional ( ~ 251.844). Podemos inferir que a rede treinada com GDM convergiu para um mínimo mais estável durante o treinamento, porém no teste a rede treinada com GD generalizou melhor os dados, o que explica o menor RMSE da GD tradicional ( ~ 0.768) quando comparado com o RMSE do GDM ( ~ 1.002)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a289a",
   "metadata": {},
   "source": [
    "## Refêrencias:\n",
    "\n",
    "[1] Momentum-based Gradient Optimizer - ML. Geeks for geeks, 2025. Disponível em: https://www.geeksforgeeks.org/ml-momentum-based-gradient-optimizer-introduction/\n",
    "\n",
    "[2] BROWNLEE, Jason. Gradient Descent With Momentum from Scratch. Machine Learning Mastery. Disponível em: https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/\n",
    "\n",
    "[3] MAHDID, Yacine. Stochastic Gradient Descent with Momentum in Python. Disponível em: https://www.youtube.com/watch?v=7EuiXb6hFAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
